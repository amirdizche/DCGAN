# -*- coding: utf-8 -*-
"""
Created on Wed Feb 13 18:50:07 2019

@author: Amirhassan
"""

# Deep Convolutional GANs

# Importing the libraries
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable

# Setting some hyperparameters
batchSize = 64 # We set the size of the batch.
imageSize = 64 # We set the size of the generated images (64x64).

# Creating the transformations
transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.

# Loading the dataset
dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.
dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.

# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

# Generator NN class

class G(nn.Module):
    
    def __init__(self):
        super(G, self).__init__() # activate inheritence
        self.main = nn.Sequential( # meta module
                nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # inverse convolution
                nn.BatchNorm2d(512), #normalize batch
                nn.ReLU(True), # Relr layer
                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # 2nd deconvolve layer,
                nn.BatchNorm2d(256), # batch normalization
                nn.ReLU(True),
                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # 3rd deconvolve layer,
                nn.BatchNorm2d(128),
                nn.ReLU(True),
                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),
                nn.BatchNorm2d(64),
                nn.ReLU(True),
                nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # last inverse conv output 3 RGB channel image
                nn.Tanh() # make sure [-1, 1]
            )
        
        def forward(self, input):
            output = self.main(input)
            return output

# Create G NN

netG = G()
netG.apply(weights_init) #initialize weights

# discriminator NN class:

class D(nn.Module):
    super(D,self).__init__()
    self.main = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # concolution layer (input = generated image)
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(64, 128, 4, 2, 1, bias = False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(128, 256, 4, 2, 1, bias = False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(256, 512, 4, 2, 1, bias = False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace = True),
            nn.Conv2d(512, 1, 4, 1, 0, bias = False),
            nn.Sigmoid()
        )
    
    def forward(self, input): # input is an image produced by generator
        output = self.main(input)
        return output.view(-1) #flatten output of convolotions 

# create D NN
netD = D()
netD.apply(weights_init)

# Training GAN

criterion = nn.BCELoss() # measure prediction error Binary cross entropy
optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))

for epoch in range(25): # go through all image in each epoch
    for i, data in enumerate(dataloader, 0): 
        # step1: update weights of NN of discriminator
        
        netD.zero_grad() # initialize gradients with 0
        # train D with real image
        real, _ = data #dont care about labels now
        input = Variable(real) # transform to torch tensor
        target = Variable(torch.ones(input.size()[0])) # real images are labeld one
        output = netD(input) # freed images to D
        errD_real = criterion(output, target) # calculate error between target and D output
        
        # train D with fake images generated by G
        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) #noise input
        fake = netG(noise) # feed noise to the G NN
        target = Variable(torch.zeros(input.size()[0])) # generate target label for fake images is zero
        output = netD(fake.detach()) # detach: removes gradient values to spped up
        errD_fake = criterion(output, target) # calculate D error for fake images
        # backpropagate total error
        errD = errD_real + errD_fake
        errD.backward() #back propagate the error, and calculate gradients
        optimizerD.step() # applies optimizer on the NN and update weights
        
        #step2: Update weights of G NN
        netG.zero_grad() 
        target = Variable(torch.ones(input.size()[0])) #target of fake images shoud be 1 here
        output = netD(fake) #feed fake images to D, Do not detach gradients need to update here
        errG = criterion(output, target) #error of generator
        errG.backward() #backpropagate error and calculate gradients
        optimizerG.step() # update weights apply optimizer
        
        # step3: print losses and sav results
        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' %(epoch, 25, i, len(dataloader), errD.data[0], errG.data[0]))
        if i % 100 == 0:
            vutils.save_image(real, '%s/real_samples_ahf.png',"./results", normalize = True)
            fake = netG(noise) # generate fake images
            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True) # We also save the fake generated images of the minibatch.
            
        







